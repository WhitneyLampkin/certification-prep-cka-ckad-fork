<h1 align="center" style="border-bottom: none">
    <a href="https://github.com/mx-ulises/certification-prep-cka-ckad" target="_blank">
        <img alt="" src="https://github.com/mx-ulises/certification-prep-cka-ckad/blob/main/assets/notes-logo.png?raw=true" style="border-radius: 50%; height: 100px;">
    </a>
    <br>
    CKA Lesson 7: Performing Node Maintenance Tasks
</h1>
<h3 align="center" style="border-bottom: none">
    Notes by: <a href="https://github.com/mx-ulises" target="_blank">Ulises Martinez</a>
</h3>
<hr />

<p align="center">
    Auto-generated by the <a href="https://github.com/WhitneyLampkin/devscriber" target="_blank">DevScriber</a> command-line tool.
</p>

<div align="center">

![GitHub language count](https://img.shields.io/github/languages/count/mx-ulises/certification-prep-cka-ckad?label=Languages)
![GitHub contributors](https://img.shields.io/github/contributors/mx-ulises/certification-prep-cka-ckad?label=Contributors&color=yellow)
![GitHub repo size](https://img.shields.io/github/repo-size/mx-ulises/certification-prep-cka-ckad?label=Repo%20Size&color=teal)
![GitHub repo file count (file type)](https://img.shields.io/github/directory-file-count/mx-ulises/certification-prep-cka-ckad?label=Files&color=purple)

</div>

## Introduction

In this lesson we will learn howw to use Metric Server to monitor Node and Pod status, how to backup and restore the ectd server, how to perform cluster node upgrades and how to have Cluster High Availability.

## Metrics Server

Kubernetes monitoring is offered by the integrated Metrics server. The server, after installation, exposes a standard API and can be used to expose custom metrics. You can use `kubectl top` to see top-like interface to provide resource usage information, but only works if you have metric server installed. To install metrics server you should do the following:

 1. Install `metrics-server`:
    ```
    kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
    ```
 1. Edit the `metrics-server` deployment:
    ```
    kubectl -n kube-system edit deployment metrics-server
    ```
 1. Add the following container arguments:
    - `--kubelet-insecure-tls`
    - `--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname`
 1. Get the information on how to get the self-signed certs and how to serve securely:
    ```
    kubectl -n kube-system logs metrics-server...
    ```

Now you can use `kubectl top`

## Maintenance of ectd

The ectd is a core kubernetes service that contaibs akk resources that have been created. It is started as a static `Pod` in the control node. If you lose etcd, means you lost all your configuration.

### ⭐ Backup of ectd

To backup ectd, you need to run `etcdctl` (with sudo access). To install it, use `sudo apt install etcd-client`. You need to use the correct API version, to ensure this, use `sudo ETCDCTL_API=3 ectdctl ...`.

To use `ectdctl` you need to specify the ectd service API endpoint, as well as a cacert, cert and key to be used. To know which certs you need to use, you can get this by doing `ps aux | grep ectd`.

Command execution:

```
    sudo ETCDCTL_API=3 ectdctl \
          --endpoints=localhost:2379 \
          --cacert <ca_path> \
          --cert <cert_path> \
          --key <key_path>  \
          snapshot save <backup_path>
```

Where:
 - `<ca_path>=/etc/kubernetes/pki/ectd/ca.crt`
 - `<cert_path>=/etc/kubernetes/pki/ectd/server.crt`
 - `<key_path>=/etc/kubernetes/pki/ectd/server.key`
 - `<backup_path>` whatever path you need for the backup

To verify the snapshot use:

```
    sudo ETCDCTL_API=3 ectdctl \
          --write-out=table \
          snapshot status <backup_path>
```

### ⭐ Restoing of ectd

The steps are:

 1. Move all static `Pod` manifests to a backup directory, this will stop all core servies:
    ```
    sudo mv /etc/kubernetes/manifests/*.yaml /tmp/kubernetes/manifest/backup
    ```
 1. Restore the backup with a non-default data-directory
    ```
        sudo ETCDCTL_API=3 ectdctl \
              snapshot restore <backup_path> \
              -- data-dir /var/lib/etcd-backup
    ```
 1. Update the ectd maniest to change the `ectd-data` HostPath volume to `/var/lib/etcd-backup`
 1. Mobe back the static `Pod` back to `/etc/kubernetes/manifests`
    ```
    sudo mv /tmp/kubernetes/manifest/backup/*.yaml /etc/kubernetes/manifests
    ```
 1. All core services will be restored after this, along with the previous ectd status.

## Cluster Node Upgrades

Kubernetes clusters can be upgraded from one to another minor versions, but not skipping (1.23 to 1.25). In documentation it can be find as "Upgrading kubeadm clusters". The steps of the proccess are:

 1. Upgrade `kubeadm`
 1. Check available versions: `kubeadm upgrade plan`
 1. Upgrade control plane node: `kubeadm upgrade apply <version>`
 1. Drain control node: `kubectl drain controlnode --ignore-daemonsets`
 1. Upgrade and restart kubelet and kubectl
 1. Bring back the control node: `kubectl uncordon controlnode`
 1. Proceed with the rest of the nodes (same steps)

 ## Clusters High Availability (HA) Options

 There are two options to enable High Availability in Control Plane:
  - Stacked Control plane nodes: It requires less infrastructure because etcd members and control plane are co-located. It require minimum of 3 stacked control plane nodes.
  - External etcd cluster: Requires more infrastructure because the control plane nodes and etcd members are separated. Etcd runs in external nodes, and require double ammount of nodes.

The HA requirements are:
 - A load balancer is needed to dristribute workload between the cluster nodes.
    - Load Balancer can be externally provided using open source software or load balancer appliance.
    - This option is not required in the CKA exam.
 - Setup shouldn't be started, just install containerd and kubectl/kubeadm tools.

Configuration:
  1. In the load balancer setup, HAProxy is running on each server to provide access to port `8443` on all IP addresses on that server. Incoming traffic on this port is forwarded to the `kube-apiserver` on port `6443`.
  1. The `keepalived` service is running on all HA nodes to provide a virtual IP address on one of the nodes.
  1. kubctl clients connects to this virtualip in port `8443`
  1. Configure the load balancer (not required for CKA)

Setting it up:
  1. Init the configuration with this parameters:
     ```
     sudo kubeadm init --control-plane-endpoint <loadbalancerIP>:8443 --upload-certs
     ```
  1. Configure network add-on:
     ```
     kubectl create -f <network-add-on-manifest>`
     ```
  1. Join the clusters with the `kubectl join` command. For control plane nodes, it has the flag `--control-plane` and we should start with them.

<p align="center" style="border-bottom: none; margin-top: 50px;">
    <a href="https://github.com/mx-ulises/certification-prep-cka-ckad" target="_blank">
        <img alt="" src="https://github.com/mx-ulises/certification-prep-cka-ckad/blob/main/assets/notes-logo.png?raw=true" style="border-radius: 50%; height: 100px;">
    </a>
</p>
